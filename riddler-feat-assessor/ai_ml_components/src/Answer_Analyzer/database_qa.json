{
    "9aaf21f2-d640-4d85-8710-b8b0ea94597e": {
        "question": "What is the difference between supervised and unsupervised learning?",
        "answer": "Supervised learning is a type of machine learning where the model is trained on labeled data, i.e., data with known outcomes. In contrast, unsupervised learning is a type of machine learning where the model is trained on unlabeled data, i.e., data without known outcomes. The goal of unsupervised learning is to identify patterns or structure in the data. Examples of supervised learning include regression and classification tasks, while examples of unsupervised learning include clustering and dimensionality reduction tasks.",
        "difficulty": "Easy"
    },
    "4c21ab07-598a-4950-a465-eb5d06b5194e": {
        "question": "What is the difference between L1 and L2 regularization?",
        "answer": "L1 and L2 regularization are techniques used to prevent overfitting in machine learning models. L1 regularization, also known as Lasso regularization, adds a penalty equal to the absolute value of the magnitude of the coefficients to the loss function. This results in some coefficients being set to zero, leading to sparse models. L2 regularization, also known as Ridge regularization, adds a penalty equal to the square of the magnitude of the coefficients to the loss function. This results in smaller coefficients, but none are set to zero. L1 regularization is useful when we want to select a subset of the features, while L2 regularization is useful when we want to reduce the size of the coefficients.",
        "difficulty": "Easy"
    },
    "5ad7223b-1b25-4c4d-8802-66ee0db19956": {
        "question": "What is cross-validation and why is it important?",
        "answer": "Cross-validation is a technique used to evaluate the performance of machine learning models and prevent overfitting. It involves dividing the data into k folds, where k-1 folds are used for training and one fold is used for testing. This process is repeated k times, and the average performance is calculated. Cross-validation is important because it provides a more reliable estimate of the model's performance than using a single train-test split. It also helps in hyperparameter tuning by providing a more robust estimate of the model's performance.",
        "difficulty": "Easy"
    },
    "62fd1886-7e7b-4667-8c7c-174bdcb828e2": {
        "question": "What is the curse of dimensionality?",
        "answer": "The curse of dimensionality is a phenomenon that occurs when the number of features or dimensions in a dataset increases. It leads to a decrease in the performance of machine learning algorithms due to the following reasons: (1) the volume of the feature space increases so fast that the available data become sparse, (2) the distance measures lose their meaning, and (3) the data become more likely to be non-linearly separable. The curse of dimensionality can be mitigated by using dimensionality reduction techniques such as PCA or t-SNE.",
        "difficulty": "Medium"
    },
    "117a2882-20ff-4a1c-aee5-c127cc75d2df": {
        "question": "What is the bias-variance tradeoff?",
        "answer": "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the tradeoff between bias and variance in a model. Bias is the error introduced by approximating a real-world problem with a simplified model, while variance is the error introduced by sensitivity to small fluctuations in the training data. A model with high bias and low variance is overly simplified and underfits the data, while a model with low bias and high variance is overly complex and overfits the data. The goal of a machine learning algorithm is to find the right balance between bias and variance that minimizes the overall error.",
        "difficulty": "Medium"
    },
    "a0c20715-b1d1-40fd-b00e-df7b29934123": {
        "question": "What is the difference between a parametric and non-parametric model?",
        "answer": "Parametric models are machine learning models that have a fixed number of parameters that need to be estimated from the data. These models assume a specific functional form for the underlying distribution. Examples of parametric models include linear regression and logistic regression. Non-parametric models, on the other hand, do not assume a specific functional form for the underlying distribution and can adapt to the complexity of the data. These models have a flexible number of parameters that can increase with the size of the data. Examples of non-parametric models include decision trees and k-nearest neighbors.",
        "difficulty": "Medium"
    },
    "87d6a4e6-1472-472c-80ba-51280fede1a3": {
        "question": "What is the difference between a generative and discriminative model?",
        "answer": "Generative models are machine learning models that estimate the joint probability distribution P(x,y) of the input features x and the output labels y. These models can be used for both classification and density estimation tasks. Discriminative models, on the other hand, estimate the conditional probability distribution P(y|x) of the output labels y given the input features x. These models are typically used for classification tasks. Generative models are useful when we want to model the underlying data distribution, while discriminative models are useful when we want to make predictions based on the data.",
        "difficulty": "Hard"
    },
    "0999b373-4b02-4599-befc-91c8b2cafdc4": {
        "question": "What is the difference between a shallow and deep neural network?",
        "answer": "A shallow neural network is a neural network with a single hidden layer, while a deep neural network is a neural network with multiple hidden layers. Deep neural networks have been shown to be more powerful than shallow neural networks in terms of their ability to learn complex representations and generalize to new data. However, deep neural networks are also more computationally expensive and require more data to train effectively.",
        "difficulty": "Hard"
    },
    "cea1c891-3ee1-4e32-aa8b-831794f0695b": {
        "question": "What is the difference between a support vector machine and a neural network?",
        "answer": "A support vector machine (SVM) is a machine learning model that finds the hyperplane that maximally separates the data into classes. SVMs are based on the principle of structural risk minimization, which aims to find the model that has the lowest expected generalization error. SVMs are typically used for classification tasks and can handle non-linearly separable data by using the kernel trick. A neural network, on the other hand, is a machine learning model that consists of interconnected nodes or neurons. Neural networks can learn complex representations and are typically used for both classification and regression tasks. Neural networks can handle non-linearly separable data by using multiple hidden layers and non-linear activation functions.",
        "difficulty": "Hard"
    },
    "b7b0578e-0357-4a3d-bd8a-0a7f6d536f69": {
        "question": "What is the difference between precision and recall?",
        "answer": "Precision and recall are evaluation metrics used in machine learning classification tasks. Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP), i.e., precision = TP / (TP + FP). Precision measures the proportion of correct positive predictions. Recall is the ratio of true positives to the sum of true positives and false negatives (FN), i.e., recall = TP / (TP + FN). Recall measures the proportion of actual positives that were correctly predicted. These metrics are often used together and can be combined into a single metric called the F1 score, which is the harmonic mean of precision and recall.",
        "difficulty": "Very Hard"
    },
    "4bcff51f-b91b-40e9-8bec-56c85982c4d7": {
        "question": "What is the difference between a confidence interval and a credible interval?",
        "answer": "A confidence interval is a range of values that contains the true value of a population parameter with a certain level of confidence. It is based on the sampling distribution of a statistic and assumes that the data are randomly sampled from a population. A credible interval, on the other hand, is a range of values that contains the true value of a population parameter with a certain level of credibility. It is based on the posterior distribution of a parameter and assumes that the data are generated from a model. Credible intervals are typically used in Bayesian statistics, while confidence intervals are typically used in frequentist statistics.",
        "difficulty": "Very Hard"
    },
    "e3b704cc-3641-494f-9817-a35c8d1cdb70": {
        "question": "What is the difference between a likelihood and a probability?",
        "answer": "A likelihood is the probability of observing the data given a set of model parameters, while a probability is the probability of an event occurring. The likelihood is used in maximum likelihood estimation, which aims to find the parameter values that maximize the likelihood of observing the data. The likelihood is not a probability distribution, as it does not sum or integrate to one. The likelihood can be used to compare different models or hypotheses by calculating the likelihood ratio, which is the ratio of the likelihoods of two models or hypotheses.",
        "difficulty": "Very Hard"
    },
    "8704969f-5e91-4e5d-a6a2-3ed071d24a69": {
        "question": "What is the difference between a Bayesian and a frequentist hypothesis test?",
        "answer": "A Bayesian hypothesis test compares the probability of the data under two hypotheses, assuming that the data are generated from a model. The hypotheses are represented as probability distributions, and the Bayes factor is used to compare them. A frequentist hypothesis test, on the other hand, compares the probability of observing the data under two hypotheses, assuming that the data are randomly sampled from a population. The hypotheses are represented as statistical tests, and the p-value is used to compare them. Bayesian hypothesis tests are more flexible than frequentist hypothesis tests, as they can incorporate prior knowledge and provide a measure of the relative evidence for each hypothesis. However, they are also more subjective, as the choice of prior distribution can affect the results.",
        "difficulty": "Very Hard"
    },
    "41bae8d2-d373-4702-9ece-587b8c031a62": {
        "question": "What is machine learning?",
        "answer": "Machine learning is a branch of artificial intelligence that focuses on using data and algorithms to enable AI to imitate the way that humans learn, gradually improving its accuracy.",
        "difficulty": "Easy"
    },
    "417e16b0-4b68-4846-93cf-01687ef91f70": {
        "question": "How does machine learning work?",
        "answer": "Machine learning models fall into three primary categories: supervised, unsupervised, and semi-supervised. Supervised learning uses labeled datasets to train algorithms to classify data or predict outcomes accurately.",
        "difficulty": "Easy"
    },
    "461e25b5-03a3-43e1-8043-7844f1851f4d": {
        "question": "What is the difference between machine learning and deep learning?",
        "answer": "Machine learning is a broader field that encompasses various techniques, including deep learning. Deep learning is a sub-field of machine learning that uses neural networks to analyze data.",
        "difficulty": "Easy"
    },
    "0b57c931-90bc-457c-a5ee-8f53a74b346e": {
        "question": "What are some common machine learning algorithms?",
        "answer": "Some common machine learning algorithms include neural networks, linear regression, logistic regression, clustering, decision trees, and random forests.",
        "difficulty": "Easy"
    },
    "066a223f-07b4-4c2b-9e6a-54ba8169ab80": {
        "question": "What are some real-world use cases of machine learning?",
        "answer": "Some real-world use cases of machine learning include speech recognition, customer service chatbots, computer vision, recommendation engines, and automated stock trading.",
        "difficulty": "Easy"
    },
    "62efaeb4-02c6-4d02-a68d-083d44d5d574": {
        "question": "What are some challenges of machine learning?",
        "answer": "Some challenges of machine learning include technological singularity, AI impact on jobs, privacy, bias and discrimination, and accountability.",
        "difficulty": "Medium"
    },
    "cf6385a0-4acb-4948-8734-a9b1b0951784": {
        "question": "How do you choose the right AI platform for machine learning?",
        "answer": "When choosing an AI platform, consider the MLOps capabilities, generative AI capabilities, and the ability to operationalize AI across your business.",
        "difficulty": "Medium"
    },
    "ffb496a5-865d-40b2-a57a-267b86a8d86e": {
        "question": "What is reinforcement machine learning?",
        "answer": "Reinforcement machine learning is a type of machine learning that uses trial and error to learn from feedback, such as rewards or penalties, to make decisions.",
        "difficulty": "Medium"
    },
    "9781dc92-e04b-459d-a353-a17eabbf58e2": {
        "question": "What is the difference between supervised and unsupervised machine learning?",
        "answer": "Supervised machine learning uses labeled datasets to train algorithms, while unsupervised machine learning uses unlabeled datasets to discover patterns and relationships.",
        "difficulty": "Medium"
    },
    "77c791bc-dce6-44c1-b86a-333fa292214f": {
        "question": "What is the role of neural networks in machine learning?",
        "answer": "Neural networks are a type of machine learning algorithm that are inspired by the structure and function of the human brain, and are particularly well-suited for tasks such as image and speech recognition.",
        "difficulty": "Medium"
    },
    "e288d677-8715-495c-83dd-e876a052c4c1": {
        "question": "How do you handle bias and discrimination in machine learning models?",
        "answer": "To handle bias and discrimination in machine learning models, it is important to use diverse and representative training data, and to regularly audit and test the models for bias.",
        "difficulty": "Hard"
    },
    "b604f23d-ce7c-479e-b30e-b07d7c8ae83a": {
        "question": "What is the role of transfer learning in machine learning?",
        "answer": "Transfer learning is a technique in machine learning where a model trained on one task is used as a starting point for training on another related task, which can improve the model's performance and reduce the need for large amounts of new training data.",
        "difficulty": "Hard"
    },
    "94afdae1-f8ba-421c-a07e-3e024b4f15ad": {
        "question": "How do you evaluate the performance of a machine learning model?",
        "answer": "To evaluate the performance of a machine learning model, it is important to use metrics such as accuracy, precision, recall, and F1 score, and to regularly test the model on new and unseen data.",
        "difficulty": "Hard"
    },
    "ed91d8cf-386b-4895-b723-ab29b8ea3a7b": {
        "question": "What is the difference between supervised and unsupervised learning?",
        "answer": "Supervised learning is a type of machine learning where the model is trained on labeled data, i.e., data with known outcomes. In contrast, unsupervised learning is a type of machine learning where the model is trained on unlabeled data, i.e., data without known outcomes. The goal of unsupervised learning is to identify patterns or structure in the data. Examples of supervised learning include regression and classification tasks, while clustering and dimensionality reduction are examples of unsupervised learning.",
        "difficulty": "Easy"
    },
    "4c038d52-0a2c-415e-90d8-9e2a7896edf7": {
        "question": "What is the difference between L1 and L2 regularization?",
        "answer": "L1 and L2 regularization are techniques used to prevent overfitting in machine learning models. L1 regularization, also known as Lasso regularization, adds a penalty equal to the absolute value of the magnitude of the coefficients to the loss function. This results in many coefficients being driven to zero, leading to sparse models. L2 regularization, also known as Ridge regularization, adds a penalty equal to the square of the magnitude of the coefficients to the loss function. This results in smaller coefficients, but none are driven to zero. L1 regularization is useful when we want to select a subset of features, while L2 regularization is useful when we want to reduce the magnitude of the coefficients.",
        "difficulty": "Easy"
    },
    "1ceb91f4-9688-4fac-a0b7-73973c4dd69a": {
        "question": "What is cross-validation and why is it important?",
        "answer": "Cross-validation is a technique used to evaluate the performance of a machine learning model and tune its hyperparameters. It involves dividing the data into k folds, where k-1 folds are used for training and one fold is used for testing. This process is repeated k times, and the performance is averaged across all k runs. Cross-validation helps to reduce the variance of the performance estimate and prevent overfitting. It is important because it provides a more reliable estimate of the model's performance on unseen data.",
        "difficulty": "Easy"
    },
    "9cedb7e1-7ed7-4cdf-a63f-45b4c446f302": {
        "question": "What is the curse of dimensionality?",
        "answer": "The curse of dimensionality refers to the phenomenon where the volume of the feature space increases exponentially with the number of features, leading to a decrease in the density of the data. This has several implications for machine learning models, including an increase in the number of required training examples, a decrease in the performance of distance-based algorithms, and an increase in the risk of overfitting. The curse of dimensionality is a fundamental challenge in high-dimensional data analysis and requires careful consideration of the dimensionality of the data and the choice of algorithms.",
        "difficulty": "Easy"
    },
    "9feb1816-fa41-4549-bcdc-b3bd020e973c": {
        "question": "What is the bias-variance tradeoff?",
        "answer": "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the tradeoff between the bias and variance of a model. Bias is the error introduced by approximating a real-world problem with a simplified model, while variance is the error introduced by the model's sensitivity to small fluctuations in the training data. A high-bias model is overly simplified and underfits the data, leading to high error on both the training and test data. A high-variance model is overly complex and overfits the data, leading to low error on the training data but high error on the test data. The bias-variance tradeoff is managed by choosing the right level of complexity for the model, which can be achieved by selecting the right model class, feature set, and hyperparameters.",
        "difficulty": "Medium"
    },
    "8d066716-45c7-4342-be73-7268d0e2f48d": {
        "question": "What is the difference between a parametric and nonparametric model?",
        "answer": "Parametric models are models that have a fixed number of parameters that are estimated from the data. The parameters represent the distribution of the data, and the model assumes a specific functional form. Nonparametric models are models that do not assume a specific functional form and have a flexible number of parameters that can grow with the data. Nonparametric models are useful when the data do not follow a specific distribution or when the number of features is large. Examples of parametric models include linear regression and logistic regression, while examples of nonparametric models include decision trees and nearest neighbors.",
        "difficulty": "Medium"
    },
    "6369f653-573f-47c5-8e80-0845eedc111f": {
        "question": "What is regularization and why is it important?",
        "answer": "Regularization is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function. The penalty term discourages large coefficients, leading to smaller and more interpretable models. Regularization is important because it helps to balance the bias-variance tradeoff and improve the generalization performance of the model. L1 and L2 regularization are common types of regularization, but other types include dropout and early stopping.",
        "difficulty": "Medium"
    },
    "5088e54d-eb18-409d-9de0-ff4d603e3b35": {
        "question": "What is a kernel function and why is it used?",
        "answer": "A kernel function is a function that takes two inputs, usually vectors in a high-dimensional space, and outputs a scalar value. Kernel functions are used in machine learning algorithms to enable computations in the high-dimensional space without explicitly mapping the data to that space. This is known as the kernel trick and enables efficient computation of inner products in the high-dimensional space. Kernel functions are used in algorithms such as support vector machines and kernel density estimation.",
        "difficulty": "Hard"
    },
    "28878e60-3fb8-4252-ae1f-7d7a467a584d": {
        "question": "What is the difference between a generative and discriminative model?",
        "answer": "Generative models are models that estimate the joint probability distribution of the features and labels, while discriminative models estimate the conditional probability distribution of the labels given the features. Generative models are useful when the data are generated by a complex process and we want to understand the underlying causes, while discriminative models are useful when we want to predict the labels given the features. Examples of generative models include naive Bayes and Gaussian mixture models, while examples of discriminative models include logistic regression and support vector machines.",
        "difficulty": "Hard"
    },
    "ac1d841e-19da-4ea2-8019-6b9d9c9075be": {
        "question": "What is the difference between a shallow and deep neural network?",
        "answer": "A shallow neural network is a neural network with one hidden layer, while a deep neural network is a neural network with multiple hidden layers. Deep neural networks can learn more complex representations and patterns in the data than shallow neural networks. However, deep neural networks require more computational resources and are more prone to overfitting. Shallow neural networks are useful when the data are simple and the number of features is small, while deep neural networks are useful when the data are complex and the number of features is large.",
        "difficulty": "Hard"
    },
    "0fe40407-2a50-46b6-be27-f49f358b8a52": {
        "question": "What is the difference between batch and online learning?",
        "answer": "Batch learning is a type of learning where the model is trained on the entire dataset at once, while online learning is a type of learning where the model is trained on one or a few examples at a time. Batch learning is useful when the dataset is small and can fit in memory, while online learning is useful when the dataset is large and cannot fit in memory or when real-time predictions are required. Online learning can also be used for active learning, where the model selects the most informative examples to label and improve its performance.",
        "difficulty": "Very Hard"
    },
    "e7edf682-870b-4ab9-aa77-9bae42100c93": {
        "question": "What is meta-learning and why is it important?",
        "answer": "Meta-learning, also known as learning to learn, is a type of machine learning where the goal is to learn a model that can quickly adapt to new tasks or domains. Meta-learning algorithms learn a prior over a distribution of tasks and use it to learn a new task or fine-tune an existing model. Meta-learning is important because it enables efficient learning of new tasks and improves the sample efficiency of machine learning models. Meta-learning has applications in few-shot learning, transfer learning, and domain adaptation.",
        "difficulty": "Very Hard"
    },
    "e255cf33-0c63-4fe8-b1b3-2310f6113da7": {
        "question": "What is the difference between a confidence and a credible interval?",
        "answer": "A confidence interval is a range of values that contains the true parameter of a population with a certain probability, usually 95%, while a credible interval is a range of values that contains the true parameter of a population with a certain probability, given the observed data and a prior distribution. Confidence intervals are used in frequentist statistics and rely on the sampling distribution of the estimator, while credible intervals are used in Bayesian statistics and rely on the posterior distribution of the parameter. Credible intervals are more informative than confidence intervals because they incorporate prior knowledge and uncertainty.",
        "difficulty": "Very Hard"
    },
    "de42a28d-bec8-42bb-ae91-caee9ac76c35": {
        "question": "What is the difference between a proper and an improper scoring rule?",
        "answer": "A scoring rule is a function that measures the accuracy of a probabilistic forecast. A proper scoring rule is a scoring rule that incentivizes the forecaster to provide their true belief, while an improper scoring rule is a scoring rule that does not incentivize the forecaster to provide their true belief. Proper scoring rules include the logarithmic score, Brier score, and continuous ranked probability score, while improper scoring rules include the quadratic score and the absolute error score. Proper scoring rules are useful for evaluating and comparing probabilistic forecasts and encouraging honest reporting.",
        "difficulty": "Very Hard"
    }
}